{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import các thư viện và cài đặt chrome options\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_category = input(\"Nhập main_category: \")\n",
    "sub_category = input(\"Nhập sub_category: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Chạy ẩn danh\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo trình duyệt WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Khai báo các danh sách để lưu thông tin sản phẩm\n",
    "product_names = []\n",
    "prices = []\n",
    "links = []\n",
    "ratings = []\n",
    "reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL của trang tìm kiếm trên Amazon\n",
    "url = f'https://www.amazon.com/s?i=specialty-aps&bbn=16225007011&rh=n%3A16225007011%2Cn%3A172456&ref=nav_em__nav_desktop_sa_intl_computer_accessories_and_peripherals_0_2_6_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm cào dữ liệu khi mỗi sản phẩm được hiển thị trên một hàng\n",
    "def findasrow(url):\n",
    "    for i in tqdm(range(1, 4), desc=\"Data From Amazon\"):\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        divs = soup.find_all('div', 'a-section a-spacing-small a-spacing-top-small')\n",
    "        temp = 0\n",
    "        for div in divs:\n",
    "            if temp == 0:\n",
    "                temp += 1\n",
    "                continue\n",
    "            product = div.find('a', 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal')\n",
    "            if product is None:\n",
    "                product_names.append(None)\n",
    "            else:\n",
    "                product_names.append(product.text)\n",
    "\n",
    "            link = div.find('a', 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal')\n",
    "            if link is None:\n",
    "                links.append(None)\n",
    "            else:\n",
    "                links.append(\"https://www.amazon.com/\" + link.get('href'))\n",
    "\n",
    "            price = div.find('span', 'a-price-whole')\n",
    "            if price is None:\n",
    "                prices.append(None)\n",
    "            else:\n",
    "                prices.append(price.text)\n",
    "\n",
    "            rate = div.find('span', 'a-icon-alt')\n",
    "            if rate is None:\n",
    "                if link is None:\n",
    "                    pass\n",
    "                ratings.append(None)\n",
    "            else:\n",
    "                ratings.append(rate.text.split()[0])\n",
    "\n",
    "            review = div.find('span', 'a-size-base s-underline-text')\n",
    "            if review is None:\n",
    "                if link is None:\n",
    "                    pass\n",
    "                reviews.append(None)\n",
    "            else:\n",
    "                reviews.append(review.text)\n",
    "\n",
    "        divs_pagination = soup.find('div', 'a-section a-text-center s-pagination-container')\n",
    "        if not divs_pagination:\n",
    "            print(\" EComplete get data\")\n",
    "            break\n",
    "        else:\n",
    "            next_page_link = divs_pagination.find('a', 's-pagination-item s-pagination-next s-pagination-button s-pagination-separator')\n",
    "            if not next_page_link:\n",
    "                print(\"Completed\")\n",
    "                break\n",
    "            else:\n",
    "                url = \"https://www.amazon.com/\" + next_page_link.get('href')\n",
    "\n",
    "    # Lưu dữ liệu vào file CSV\n",
    "    print(\"Length of products: \", len(product_names))\n",
    "    print(\"Length of prices: \", len(prices))\n",
    "    print(\"Length of links: \", len(links))\n",
    "    print(\"Length of ratings: \", len(ratings))\n",
    "    print(\"Length of reviews: \", len(reviews))\n",
    "\n",
    "    filename = f\"part1_data_{main_category}_{sub_category}.csv\"\n",
    "    with open(filename, 'w', newline=\"\", encoding='utf-8') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow(['MainCategory', 'SubCategory', 'Product Name', 'Price', 'Link', 'Rating', 'Review'])\n",
    "            for product, price, link, rate, review in zip(product_names, prices, links, ratings, reviews):\n",
    "                csv_writer.writerow([main_category, sub_category, product, price, link, rate, review])\n",
    "\n",
    "    print(f\"SAVED THE FILE!!! part1_data_{main_category}_{sub_category}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm cào dữ liệu khi mỗi sản phẩm được hiển thị trong một ô\n",
    "def crawlastiles(url):\n",
    "    for i in tqdm(range(1, 41), desc=\"Data From Amazon\"):\n",
    "        driver.get(url)\n",
    "        #time.sleep(8)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        divs = soup.find_all('div', 'sg-col-4-of-24 sg-col-4-of-12 s-result-item s-asin sg-col-4-of-16 sg-col s-widget-spacing-small sg-col-4-of-20')\n",
    "        temp = 0\n",
    "        for div in divs:\n",
    "            if temp == 0:\n",
    "                temp += 1\n",
    "                continue\n",
    "            product = div.find('a', 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal')\n",
    "            if product is None:\n",
    "                product_names.append(None)\n",
    "            else:\n",
    "                product_names.append(product.text)\n",
    "            \n",
    "            link = div.find('a', 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal')\n",
    "            if link is None:\n",
    "                links.append(None)\n",
    "            else:\n",
    "                links.append(\"https://www.amazon.com/\" + link.get('href'))\n",
    "            \n",
    "            price = div.find('span', 'a-price-whole')\n",
    "            if price is None:\n",
    "                prices.append(None)\n",
    "            else:\n",
    "                prices.append(price.text)\n",
    "            \n",
    "            rate = div.find('span', 'a-icon-alt')\n",
    "            if rate is None:\n",
    "                if link is None:\n",
    "                    pass\n",
    "                ratings.append(None)\n",
    "            else:\n",
    "                ratings.append(rate.text.split()[0])\n",
    "            \n",
    "            reviewnumber = div.find('span','a-size-base s-underline-text')\n",
    "            if reviewnumber is None:\n",
    "                if link is None:\n",
    "                    pass\n",
    "                reviews.append(None)\n",
    "            else:\n",
    "                reviews.append(reviewnumber.text)\n",
    "\n",
    "        divs_pagination = soup.find('div', 'a-section a-text-center s-pagination-container')\n",
    "        if not divs_pagination:\n",
    "            print(\"Complete get data\")\n",
    "            break\n",
    "        else:\n",
    "            next_page_link = divs_pagination.find('a','s-pagination-item s-pagination-next s-pagination-button s-pagination-separator')\n",
    "            if not next_page_link:\n",
    "                print(\"Completed, end of pagination\")\n",
    "                break\n",
    "            else:\n",
    "                url = \"https://www.amazon.com\" + next_page_link.get('href')\n",
    "                driver.get(url)\n",
    "\n",
    "    # Lưu dữ liệu vào file CSV\n",
    "    print(\"Length of products: \", len(product_names))\n",
    "    print(\"Length of prices: \", len(prices))\n",
    "    print(\"Length of links: \", len(links))\n",
    "    print(\"Length of ratings: \", len(ratings))\n",
    "    print(\"Length of reviews: \", len(reviews))\n",
    "    \n",
    "    filename = f\"part1_data_{main_category}_{sub_category}.csv\"\n",
    "    with open(filename, 'w', newline=\"\", encoding='utf-8') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow(['MainCategory', 'SubCategory', 'Product Name', 'Price', 'Link', 'Rating', 'Review'])\n",
    "            for product, price, link, rate, review in zip(product_names, prices, links, ratings, reviews):\n",
    "                csv_writer.writerow([main_category, sub_category, product, price, link, rate, review])\n",
    "\n",
    "    print(f\"SAVED THE FILE!!! part1_data_{main_category}_{sub_category}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang thực hiện crawl theo ô sản phẩm ()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data From Amazon:   0%|          | 0/40 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete get data\n",
      "Length of products:  0\n",
      "Length of prices:  0\n",
      "Length of links:  0\n",
      "Length of ratings:  0\n",
      "Length of reviews:  0\n",
      "SAVED THE FILE!!! part1_data_Computer_Computer Accessories & Peripherals.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra cách trình bày sản phẩm để quyết định cách thực hiện việc cào dữ liệu\n",
    "driver.get(url)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "divs = soup.find_all('div', 'sg-col-4-of-24 sg-col-4-of-12 s-result-item s-asin sg-col-4-of-16 sg-col s-widget-spacing-small sg-col-4-of-20')\n",
    "\n",
    "if not divs:\n",
    "    print(\"Đang thực hiện crawl theo hàng sản phẩm()...\")\n",
    "    findasrow(url)  # Truyền url vào hàm findasrow()\n",
    "else:\n",
    "    print(\"Đang thực hiện crawl theo ô sản phẩm ()...\")\n",
    "    crawlastiles(url)  # Truyền url vào hàm crawlastiles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PHẦN 2: CÀO CHI TIẾT SẢN PHẨM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shipping_days(estimated_delivery_date_str, crawl_date_str):\n",
    "    try:\n",
    "        estimated_delivery_date = datetime.datetime.strptime(estimated_delivery_date_str, '%A, %B %d')\n",
    "        crawl_date = datetime.datetime.strptime(crawl_date_str, '%A, %B %d')\n",
    "        return (estimated_delivery_date - crawl_date).days\n",
    "    except ValueError:\n",
    "        return 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r'[\\u200e]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailproduct_bucket(soup, divs2I):\n",
    "    dimension = asinid = datefirstavailable = manufactures = country = sellerrank = color = modelnumber = weight = price = priceamzship = eic = shipping_days = 'None'\n",
    "\n",
    "    divs3 = divs2I.find_all('span', class_='a-text-bold')\n",
    "    for div in divs3:\n",
    "        if 'Package Dimensions' in div.text.strip() or 'Dimensions' in div.text.strip():\n",
    "            div4 = div.find_next_sibling('span')\n",
    "            dimension = clean_text(div4.text.strip() if div4 else 'None')\n",
    "        elif 'Item model number' in div.text.strip():\n",
    "            div4 = div.find_next_sibling('span')\n",
    "            modelnumber = clean_text(div4.text.strip() if div4 else 'None')\n",
    "        elif 'Date First Available' in div.text.strip():\n",
    "            div4 = div.find_next_sibling('span')\n",
    "            datefirstavailable = clean_text(div4.text.strip() if div4 else 'None')\n",
    "        elif 'Manufacturer' in div.text.strip():\n",
    "            div4 = div.find_next_sibling('span')\n",
    "            manufactures = clean_text(div4.text.strip() if div4 else 'None')\n",
    "        elif 'ASIN' in div.text.strip():\n",
    "            div4 = div.find_next_sibling('span')\n",
    "            asinid = clean_text(div4.text.strip() if div4 else 'None')\n",
    "        elif 'Country of Origin' in div.text.strip():\n",
    "            div4 = div.find_next_sibling('span')\n",
    "            country = clean_text(div4.text.strip() if div4 else 'None')\n",
    "        elif 'Best Sellers Rank' in div.text.strip():\n",
    "            parent_tag = div.parent\n",
    "            sellerrank = clean_text(parent_tag.get_text(strip=True))\n",
    "        elif 'Color' in div.text.strip():\n",
    "            div4 = div.find_next_sibling('span')\n",
    "            color = clean_text(div4.text.strip() if div4 else 'Not Given')\n",
    "\n",
    "    divs2Ia = soup.find('div', class_='a-popover-preload', id='a-popover-agShipMsgPopover')\n",
    "    if (divs2Ia):\n",
    "        table = divs2Ia.find('table', class_='a-lineitem')\n",
    "        if (table):\n",
    "            lines = table.find_all('td', class_='a-span9 a-text-left')\n",
    "            for td in lines:\n",
    "                if 'Price' in td.text:\n",
    "                    divsprice = td.find_next_sibling('td', class_='a-span2 a-text-right')\n",
    "                    price = clean_text(divsprice.text.strip() if divsprice else 'None')\n",
    "                if 'AmazonGlobal Shipping' in td.text:\n",
    "                    divsamzship = td.find_next_sibling('td', class_='a-span2 a-text-right')\n",
    "                    priceamzship = clean_text(divsamzship.text.strip() if divsamzship else 'None')\n",
    "                if 'Estimated Import Charges' in td.text:\n",
    "                    diveic = td.find_next_sibling('td', class_='a-span2 a-text-right')\n",
    "                    eic = clean_text(diveic.text.strip() if diveic else 'None')\n",
    "\n",
    "    estimated_delivery_date_tag = soup.find('span', class_='a-text-bold')\n",
    "    estimated_delivery_date = clean_text(estimated_delivery_date_tag.text.strip() if estimated_delivery_date_tag else 'N/A')\n",
    "    crawl_date = datetime.datetime.now().strftime('%A, %B %d')\n",
    "    shipping_days = calculate_shipping_days(estimated_delivery_date, crawl_date)\n",
    "    \n",
    "    return {\n",
    "        \"Dimension\": dimension,\n",
    "        \"ASIN\": asinid,\n",
    "        \"Date First Available\": datefirstavailable,\n",
    "        \"Manufacturer\": manufactures,\n",
    "        \"Country of Origin\": country,\n",
    "        \"Best Sellers Rank\": sellerrank,\n",
    "        \"Color\": color,\n",
    "        \"Item Model Number\": modelnumber,\n",
    "        \"Item Weight\": weight,\n",
    "        \"Price\": price,\n",
    "        \"AmazonGlobal Shipping\": priceamzship,\n",
    "        \"Estimated Import Charges\": eic,\n",
    "        \"Day Delivered\": str(shipping_days)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailproduct_table(soup, divs2):\n",
    "    dimension = asinid = datefirstavailable = manufactures = country = sellerrank = color = modelnumber = weight = price = priceamzship = eic = shipping_days = 'None'\n",
    "\n",
    "    divs3 = divs2.find_all('th', class_='a-color-secondary a-size-base prodDetSectionEntry')\n",
    "    for div in divs3:\n",
    "        if 'Manufacturer' in div.text.strip():\n",
    "            divs7 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')\n",
    "            manufactures = clean_text(divs7.text.strip() if divs7 else 'None')\n",
    "        if 'Dimensions' in div.text.strip():\n",
    "            divs4 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')\n",
    "            dimension = clean_text(divs4.text.strip() if divs4 else 'None')\n",
    "        if 'ASIN' in div.text.strip():\n",
    "            divs5 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')\n",
    "            asinid = clean_text(divs5.text.strip() if divs5 else 'None')\n",
    "        if 'Date First Available' in div.text.strip():\n",
    "            divs6 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')\n",
    "            datefirstavailable = clean_text(divs6.text.strip() if divs6 else 'None')\n",
    "        if 'Country of Origin' in div.text.strip():\n",
    "            divs8 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')\n",
    "            country = clean_text(divs8.text.strip() if divs8 else 'None')\n",
    "        if 'Best Sellers Rank' in div.text.strip():\n",
    "            divs9 = div.find_next_sibling('td')\n",
    "            sellerrank = clean_text(divs9.text.strip() if divs9 else 'None')\n",
    "        if 'Color' in div.text.strip():\n",
    "            divs10 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')\n",
    "            color = clean_text(divs10.text.strip() if divs10 else 'Not Given')\n",
    "        if 'Item model number' in div.text.strip():\n",
    "            divs11 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')\n",
    "            modelnumber = clean_text(divs11.text.strip() if divs11 else 'Not Given')\n",
    "        if 'Item Weight' in div.text.strip():\n",
    "            divs12 = div.find_next_sibling('td', class_='a-size-base prodDetAttrValue')\n",
    "            weight = clean_text(divs12.text.strip() if divs12 else 'Not Given')\n",
    "\n",
    "    divs2a = soup.find('div', class_='a-popover-preload', id='a-popover-agShipMsgPopover')\n",
    "    if (divs2a):\n",
    "        table = divs2a.find('table', class_='a-lineitem')\n",
    "        if (table):\n",
    "            lines = table.find_all('td', class_='a-span9 a-text-left')\n",
    "            for td in lines:\n",
    "                if 'Price' in td.text:\n",
    "                    divsprice = td.find_next_sibling('td', class_='a-span2 a-text-right')\n",
    "                    price = clean_text(divsprice.text.strip() if divsprice else 'None')\n",
    "                if 'AmazonGlobal Shipping' in td.text:\n",
    "                    divsamzship = td.find_next_sibling('td', class_='a-span2 a-text-right')\n",
    "                    priceamzship = clean_text(divsamzship.text.strip() if divsamzship else 'None')\n",
    "                if 'Estimated Import Charges' in td.text:\n",
    "                    diveic = td.find_next_sibling('td', class_='a-span2 a-text-right')\n",
    "                    eic = clean_text(diveic.text.strip() if diveic else 'None')\n",
    "\n",
    "    estimated_delivery_date_tag = soup.find('span', class_='a-text-bold')\n",
    "    estimated_delivery_date = clean_text(estimated_delivery_date_tag.text.strip() if estimated_delivery_date_tag else 'N/A')\n",
    "    crawl_date = datetime.datetime.now().strftime('%A, %B %d')\n",
    "    shipping_days = calculate_shipping_days(estimated_delivery_date, crawl_date)\n",
    "    \n",
    "    return {\n",
    "        \"Dimension\": dimension,\n",
    "        \"ASIN\": asinid,\n",
    "        \"Date First Available\": datefirstavailable,\n",
    "        \"Manufacturer\": manufactures,\n",
    "        \"Country of Origin\": country,\n",
    "        \"Best Sellers Rank\": sellerrank,\n",
    "        \"Color\": color,\n",
    "        \"Item Model Number\": modelnumber,\n",
    "        \"Item Weight\": weight,\n",
    "        \"Price\": price,\n",
    "        \"AmazonGlobal Shipping\": priceamzship,\n",
    "        \"Estimated Import Charges\": eic,\n",
    "        \"Day Delivered\": str(shipping_days)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = pd.read_csv(f'part1_data_{main_category}_{sub_category}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\")\n",
    "chrome_options.add_argument(\"--log-level=3\")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_limit = 3  # Đặt giới hạn số lần thử lại\n",
    "\n",
    "count = 0\n",
    "for urldetail in url_list['Link']:\n",
    "    count += 1\n",
    "    retry_count = 0\n",
    "    success = False\n",
    "    \n",
    "    print(f\"Lần lấy thông tin thứ: {count}\")\n",
    "\n",
    "    while retry_count < retry_limit and not success:\n",
    "        try:\n",
    "            driver.get(urldetail)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, 'prodDetails')) or \n",
    "                EC.presence_of_element_located((By.ID, 'detailBulletsWrapper_feature_div'))\n",
    "            )\n",
    "            success = True\n",
    "        except TimeoutException:\n",
    "            retry_count += 1\n",
    "            print(f\"Timeout while waiting for page to load: {urldetail}. Retrying {retry_count}/{retry_limit}\")\n",
    "            time.sleep(5)  # Chờ 5 giây trước khi thử lại\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed to load page after {retry_limit} attempts: {urldetail}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    divs2 = soup.find('div', id='prodDetails')\n",
    "    divs2I = soup.find('div', id='detailBulletsWrapper_feature_div')\n",
    "\n",
    "    if divs2:\n",
    "        result = detailproduct_table(soup, divs2)\n",
    "    elif divs2I:\n",
    "        result = detailproduct_bucket(soup, divs2I)\n",
    "    \n",
    "    if result:\n",
    "        results.append(result)\n",
    "    \n",
    "    print(result)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart2_dataDetail_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_category\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub_category\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m csv_file:\n\u001b[1;32m----> 3\u001b[0m     csv_writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(csv_file, fieldnames\u001b[38;5;241m=\u001b[39m\u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m      4\u001b[0m     csv_writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[0;32m      5\u001b[0m     csv_writer\u001b[38;5;241m.\u001b[39mwriterows(results)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "filename = f\"part2_dataDetail_{main_category}_{sub_category}.csv\"\n",
    "with open(filename, 'w', newline=\"\", encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=result.keys())\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(results)\n",
    "\n",
    "print(f\"Completed scraping and saved to {filename}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
